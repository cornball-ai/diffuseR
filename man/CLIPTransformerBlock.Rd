% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/text_encoder.R
\name{CLIPTransformerBlock}
\alias{CLIPTransformerBlock}
\title{CLIP Transformer Block}
\usage{
CLIPTransformerBlock(embed_dim, num_heads, mlp_dim, gelu_type = "tanh")
}
\arguments{
\item{embed_dim}{Embedding dimension}

\item{num_heads}{Number of attention heads}

\item{mlp_dim}{MLP hidden dimension}

\item{gelu_type}{GELU variant: "tanh", "quick", or "exact"}
}
\description{
Pre-norm transformer block with attention and MLP (HuggingFace style)
}
\keyword{internal}
