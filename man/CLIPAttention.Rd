% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/text_encoder.R
\name{CLIPAttention}
\alias{CLIPAttention}
\title{CLIP Attention Block}
\usage{
CLIPAttention(embed_dim, num_heads)
}
\arguments{
\item{embed_dim}{Embedding dimension}

\item{num_heads}{Number of attention heads}
}
\description{
Multi-head self-attention with separate Q/K/V projections (HuggingFace style)
}
\keyword{internal}
