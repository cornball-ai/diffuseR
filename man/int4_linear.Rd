% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/gpu_poor.R
\name{int4_linear}
\alias{int4_linear}
\title{INT4 Linear Layer}
\usage{
int4_linear(
  in_features,
  out_features,
  bias = TRUE,
  device = "cuda",
  dtype = torch::torch_float16()
)
}
\arguments{
\item{in_features}{Integer. Size of each input sample.}

\item{out_features}{Integer. Size of each output sample.}

\item{bias}{Logical. If TRUE, adds a learnable bias (stored in float16).}

\item{device}{Character. Device for the layer.}

\item{dtype}{torch_dtype. Data type for dequantized operations.}
}
\value{
nn_module with INT4 weight storage.
}
\description{
A linear layer that stores weights in INT4 format and dequantizes on-the-fly
during forward pass. This enables running large models on limited VRAM by
keeping weights compressed on GPU.
}
\details{
The layer stores:
- `weight_packed`: uint8 tensor with packed INT4 values
- `weight_scales`: float32 tensor with per-block scales
- `weight_shape`: original weight shape
- `bias`: optional float16 bias

During forward(), weights are dequantized to the target dtype, the matmul
is performed, and the dequantized tensor is freed. This allows ~40GB models
to run with ~10GB of VRAM for weights.
}
