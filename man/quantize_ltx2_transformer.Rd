% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/gpu_poor.R
\name{quantize_ltx2_transformer}
\alias{quantize_ltx2_transformer}
\title{Quantize LTX-2 Transformer to INT4}
\usage{
quantize_ltx2_transformer(
  model_id = "Lightricks/LTX-2",
  output_dir = NULL,
  block_size = 64L,
  force = FALSE,
  download = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{model_id}{Character. HuggingFace model ID (default "Lightricks/LTX-2").}

\item{output_dir}{Character. Directory to save quantized weights. Default uses
`tools::R_user_dir("diffuseR", "cache")`.}

\item{block_size}{Integer. Block size for INT4 quantization (default 64).}

\item{force}{Logical. Re-quantize even if cached file exists.}

\item{download}{Logical. If TRUE, download model from HuggingFace if not cached.}

\item{verbose}{Logical. Print progress.}
}
\details{
LTX-2 is a 19B parameter model (~40GB in BF16). INT4 quantization reduces
this to ~5.7GB, fitting in 16GB VRAM with room for activations.

The function:
1. Uses hfhub to locate/download the model from HuggingFace
2. Loads each safetensor shard
3. Quantizes all weights to INT4 (block-wise, 64 values per scale)
4. Saves as safetensors file
}
\value{
Character. Path to the quantized weights file.
}
\description{
Downloads (if needed) and quantizes the LTX-2 19B transformer to INT4
format. The quantized weights are cached for future use.
}
\examples{
\dontrun{
# Quantize and cache (first run takes ~10-20 minutes)
weights_path <- quantize_ltx2_transformer()

# Load quantized weights
q <- load_int4_weights(weights_path)

# Dequantize specific layer on GPU
layer_weight <- dequantize_int4(q[["transformer_blocks.0.attn1.to_q.weight"]],
                                 device = "cuda", dtype = torch_float16())
}
}
