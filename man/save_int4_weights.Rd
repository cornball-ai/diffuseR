% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/gpu_poor.R
\name{save_int4_weights}
\alias{save_int4_weights}
\title{Save INT4 Quantized Weights}
\usage{
save_int4_weights(
  quantized_params,
  path,
  max_shard_size = 2e+09,
  verbose = TRUE
)
}
\arguments{
\item{quantized_params}{List of quantized parameters from `quantize_model_int4()`.}

\item{path}{Character. Base path for safetensors files. If multiple shards needed,
files will be named `path-00001-of-NNNNN.safetensors`.}

\item{max_shard_size}{Numeric. Maximum bytes per shard (default 2GB to avoid R
integer overflow issues).}

\item{verbose}{Logical. Print progress.}
}
\value{
Invisible character vector of saved file paths.
}
\description{
Saves INT4 quantized weights to disk as sharded safetensors files.
}
\details{
Weights are saved in safetensors format with the following structure:
\itemize{
  \item `{name}::packed` - uint8 tensor with packed INT4 values
  \item `{name}::scales` - float32 tensor with per-block scales
  \item `{name}::shape` - int64 tensor with original shape
}

Large models are automatically sharded to avoid R's 2GB vector limit.
The block size is fixed at 64 (standard for INT4 quantization).
}
\examples{
\dontrun{
q <- quantize_model_int4(model)
save_int4_weights(q, "model_int4.safetensors")
}
}
