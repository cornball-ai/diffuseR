% Generated by rhydrogen: do not edit by hand
% Please edit documentation in R/txt2vid_ltx2.R
\name{txt2vid_ltx2}
\alias{txt2vid_ltx2}
\title{Generate Video from Text Prompt using LTX-2}
\usage{
txt2vid_ltx2(
  prompt,
  negative_prompt = NULL,
  width = 768L,
  height = 512L,
  num_frames = 121L,
  fps = 24,
  num_inference_steps = 8L,
  guidance_scale = 4,
  memory_profile = "auto",
  text_backend = "gemma3",
  text_model_path = NULL,
  text_api_url = NULL,
  vae = NULL,
  dit = NULL,
  connectors = NULL,
  seed = NULL,
  output_file = NULL,
  output_format = "mp4",
  return_latents = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{prompt}{Character. Text prompt describing the video to generate.}

\item{negative_prompt}{Character. Optional negative prompt.}

\item{width}{Integer. Video width in pixels (default 768).}

\item{height}{Integer. Video height in pixels (default 512).}

\item{num_frames}{Integer. Number of frames to generate (default 121).}

\item{fps}{Numeric. Frames per second (default 24).}

\item{num_inference_steps}{Integer. Number of denoising steps (default 8 for distilled).}

\item{guidance_scale}{Numeric. CFG scale (default 4.0).}

\item{memory_profile}{Character or list. Memory profile: "auto" for auto-detection, or a
profile from `ltx2_memory_profile()`.}

\item{text_backend}{Character. Text encoding backend: "gemma3" (native), "api",
"precomputed", or "random".}

\item{text_model_path}{Character. Path to Gemma3 model (for "gemma3" backend). Supports glob
patterns.}

\item{text_api_url}{Character. URL for text encoding API (if backend = "api").}

\item{vae}{Optional. Pre-loaded VAE module.}

\item{dit}{Optional. Pre-loaded DiT transformer module.}

\item{connectors}{Optional. Pre-loaded text connectors module.}

\item{seed}{Integer. Random seed for reproducibility.}

\item{output_file}{Character. Path to save output video (NULL for no save).}

\item{output_format}{Character. Output format: "mp4", "gif", or "frames".}

\item{return_latents}{Logical. If TRUE, also return final latents.}

\item{verbose}{Logical. Print progress messages.}
}
\value{
A list with:
  - `video`: Array of video frames [frames, height, width, channels]
  - `latents`: (if return_latents=TRUE) Final latent tensor
  - `metadata`: Generation metadata
}
\description{
Generates video using the LTX-2 diffusion transformer model.
}
\examples{
\dontrun{
# Basic usage
result <- txt2vid_ltx2("A cat walking on a beach at sunset")

# With specific settings
result <- txt2vid_ltx2(
  prompt = "A timelapse of clouds moving over mountains",
  width = 512,
  height = 320,
  num_frames = 61,
  num_inference_steps = 8,
  seed = 42,
  output_file = "clouds.mp4"
)
}
}
