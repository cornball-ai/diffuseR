<!-- Generated by fyi::use_fyi_md() on 2026-01-11 -->
<!-- Regenerate with: fyi::use_fyi_md("diffuseR") -->

# fyi: diffuseR

## Exported Functions (diffuseR::)

| Function | Arguments |
|----------|-----------|
| `auto_devices` | model, strategy |
| `CLIPTokenizer` | prompt, merges, vocab_file, pad_token |
| `ddim_scheduler_create` | num_train_timesteps, num_inference_steps, eta, beta_schedule, beta_start, beta_end, rescale_betas_zero_snr, dtype, device |
| `ddim_scheduler_step` | model_output, timestep, sample, schedule, eta, use_clipped_model_output, thresholding, generator, variance_noise, clip_sample, set_alpha_to_one, prediction_type, dtype, device |
| `download_component` | model_name, component, device, overwrite, show_progress |
| `filename_from_prompt` | prompt, datetime |
| `img2img` | input_image, prompt, negative_prompt, img_dim, model_name, pipeline, devices, unet_dtype_str, download_models, scheduler, num_inference_steps, strength, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ... |
| `load_decoder_weights` | native_decoder, torchscript_path, verbose |
| `load_model_component` | component, model_name, device, unet_dtype_str, download, use_native |
| `load_pipeline` | model_name, m2d, i2i, unet_dtype_str, use_native_decoder, use_native_text_encoder, ... |
| `load_text_encoder_weights` | native_encoder, torchscript_path, verbose |
| `load_text_encoder2_weights` | native_encoder, torchscript_path, verbose |
| `models2devices` | model_name, devices, unet_dtype_str, download_models |
| `post_quant_conv` | x, dtype, device |
| `preprocess_image` | input, device, width, height |
| `quant_conv` | x, dtype, device |
| `save_image` | img, save_to, normalize |
| `scheduler_add_noise` | original_latents, noise, timestep, scheduler_obj |
| `text_encoder_native` | vocab_size, context_length, embed_dim, num_layers, num_heads, mlp_dim, apply_final_ln |
| `text_encoder2_native` | vocab_size, context_length, embed_dim, num_layers, num_heads, mlp_dim |
| `txt2img` | prompt, model_name, ... |
| `txt2img_sd21` | prompt, negative_prompt, img_dim, pipeline, devices, unet_dtype_str, download_models, scheduler, timesteps, initial_latents, num_inference_steps, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ... |
| `txt2img_sdxl` | prompt, negative_prompt, img_dim, pipeline, devices, unet_dtype_str, download_models, scheduler, timesteps, initial_latents, num_inference_steps, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ... |
| `vae_decoder_native` | latent_channels, out_channels |


## Internal Functions (diffuseR:::)

| Function | Arguments |
|----------|-----------|
| `.build_fallback_devices` | model, strategy |
| `CLIPAttention` | embed_dim, num_heads |
| `CLIPMLP` | in_dim, hidden_dim, gelu_type |
| `CLIPTransformerBlock` | embed_dim, num_heads, mlp_dim, gelu_type |
| `detect_text_encoder_architecture` | torchscript_path |
| `download_model` | model_name, devices, unet_dtype_str, overwrite, show_progress, download_models |
| `get_component_file_path` | component, model_dir, device, unet_dtype_str |
| `get_required_components` | model_name |
| `load_text_encoders` | model_name, device, download |
| `quick_gelu` | x |
| `rescale_zero_terminal_snr` | betas |
| `setup_dtype` | devices, unet_dtype_str |
| `standardize_devices` | devices, required_components |
| `token_get_pairs` | symbols |
| `token_merge_pair_once` | symbols, bigram |
| `VAEAttentionBlock` | channels |
| `VAEMidBlock` | channels |
| `VAEResnetBlock` | in_channels, out_channels |
| `VAEUpBlock` | in_channels, out_channels, num_resnets, add_upsample |


## Options

No options found in `diffuseR`.



# Documentation: diffuseR

## auto_devices

*Auto-Configure Device Assignment*

```
_A_u_t_o-_C_o_n_f_i_g_u_r_e _D_e_v_i_c_e _A_s_s_i_g_n_m_e_n_t

_D_e_s_c_r_i_p_t_i_o_n:

     Automatically determines optimal device configuration for
     diffusion model components based on available VRAM and GPU
     architecture. Uses gpuctl for detection if available, otherwise
     falls back to sensible defaults.

_U_s_a_g_e:

     auto_devices(model, strategy)
     
_A_r_g_u_m_e_n_t_s:

   model: Character. Model type: "sd21" or "sdxl".

strategy: Character. Memory strategy: "auto" (default), "full_gpu",
          "unet_gpu", or "cpu_only". See Details.

_D_e_t_a_i_l_s:

     Strategies: \describe{ \item{"auto"}{Detect VRAM and choose best
     strategy (requires gpuctl)} \item{"full_gpu"}{All components on
     CUDA (16GB+ VRAM for SDXL)} \item{"unet_gpu"}{Only unet on CUDA,
     rest on CPU (8GB+ VRAM)} \item{"cpu_only"}{All components on CPU}
     }

     If gpuctl is not installed, "auto" falls back to "unet_gpu" which
     works on most modern GPUs (8GB+ VRAM).

     On Blackwell GPUs (RTX 50xx), "unet_gpu" is forced due to
     TorchScript compatibility issues, regardless of available VRAM.

_V_a_l_u_e:

     A named list of device assignments suitable for
     `models2devices()`.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     # Auto-detect best configuration
     devices <- auto_devices("sdxl")
     
     # Use with models2devices
     m2d <- models2devices("sdxl", devices = auto_devices("sdxl"))
     
     # Force CPU-only
     devices <- auto_devices("sdxl", strategy = "cpu_only")
     ## End(Not run)
     
```

## CLIPAttention

*CLIP Attention Block*

```
_C_L_I_P _A_t_t_e_n_t_i_o_n _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     Multi-head self-attention with separate Q/K/V projections
     (HuggingFace style)

_A_r_g_u_m_e_n_t_s:

embed_dim: Embedding dimension

num_heads: Number of attention heads

```

## CLIPMLP

*CLIP MLP Block*

```
_C_L_I_P _M_L_P _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     Feed-forward network with configurable activation

_A_r_g_u_m_e_n_t_s:

  in_dim: Input dimension

hidden_dim: Hidden dimension

gelu_type: GELU variant: "tanh" (tanh approximation), "quick"
          (QuickGELU), "exact" (standard GELU)

```

## CLIPTokenizer

```
_C_L_I_P_T_o_k_e_n_i_z_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     CLIPTokenizer

_U_s_a_g_e:

     CLIPTokenizer(prompt, merges, vocab_file, pad_token)
     
_V_a_l_u_e:

     A 2D torch tensor of shape c(1, 77) containing the token IDs.

```

## CLIPTransformerBlock

*CLIP Transformer Block*

```
_C_L_I_P _T_r_a_n_s_f_o_r_m_e_r _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     Pre-norm transformer block with attention and MLP (HuggingFace
     style)

_A_r_g_u_m_e_n_t_s:

embed_dim: Embedding dimension

num_heads: Number of attention heads

 mlp_dim: MLP hidden dimension

gelu_type: GELU variant: "tanh", "quick", or "exact"

```

## ddim_scheduler_create

*Create a DDIM Scheduler*

```
_C_r_e_a_t_e _a _D_D_I_M _S_c_h_e_d_u_l_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Creates a Denoising Diffusion Implicit Models (DDIM) scheduler for
     use with diffusion models. DDIM schedulers provide a deterministic
     sampling process that offers faster inference compared to DDPM
     while maintaining high quality outputs.

_U_s_a_g_e:

     ddim_scheduler_create(num_train_timesteps, num_inference_steps, eta, beta_schedule, beta_start, beta_end, rescale_betas_zero_snr, dtype, device)
     
_A_r_g_u_m_e_n_t_s:

num_train_timesteps: Integer. The number of diffusion steps used to
          train the model. Default: 1000

num_inference_steps: Integer. The number of diffusion steps used for
          inference. Fewer steps typically means faster inference at
          the cost of sample quality. Default: 50

     eta: Numeric. Controls the amount of stochasticity. When eta=0,
          the sampling process is deterministic. When eta=1, the
          sampling process is equivalent to DDPM. Default: 0

beta_schedule: Character. The beta schedule to use. Options are:
          \describe{ \item{"linear"}{Linear beta schedule from
          beta_start to beta_end} \item{"scaled_linear"}{Scaled linear
          schedule, generally gives better results}
          \item{"cosine"}{Cosine schedule that approaches zero
          smoothly} } Default: "linear"

beta_start: Numeric. The starting value for the beta schedule. Default:
          0.00085

beta_end: Numeric. The final value for the beta schedule. Default:
          0.012

rescale_betas_zero_snr: Logical. If TRUE, rescales the beta values

   dtype: The data type to use for computations. Default is
          torch_float32(). Options are torch_float16() and
          torch_float32().

  device: The device to use for computations. Options are
          torch_device("cpu"), torch_device("cuda").

_D_e_t_a_i_l_s:

     DDIM (Denoising Diffusion Implicit Models) was introduced by Song
     et al. (2020) as an extension to DDPM (Denoising Diffusion
     Probabilistic Models). It offers a deterministic sampling process
     and allows for controlling the number of inference steps
     independently from the training process.

     The scheduler contains the noise schedule and methods for
     computing alpha, beta, and other parameters used in the diffusion
     process.

_V_a_l_u_e:

     A DDIM scheduler object that can be used with diffusion models to
     generate samples.

_R_e_f_e_r_e_n_c_e_s:

     Song, J., Meng, C., & Ermon, S. (2020). "Denoising Diffusion
     Implicit Models." \url{https://arxiv.org/abs/2010.02502}

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     # Create a DDIM scheduler with custom parameters
     scheduler <- ddim_scheduler_create(
       num_train_timesteps = 1000,
       num_inference_steps = 30,
       eta = 0.5,
       beta_schedule = "scaled_linear"
     )
     ## End(Not run)
     
```

## ddim_scheduler_step

*Perform a DDIM scheduler step*

```
_P_e_r_f_o_r_m _a _D_D_I_M _s_c_h_e_d_u_l_e_r _s_t_e_p

_D_e_s_c_r_i_p_t_i_o_n:

     Performs a single denoising step using the DDIM (Denoising
     Diffusion Implicit Models) algorithm. This function takes the
     output from a diffusion model at a specific timestep and computes
     the previous (less noisy) sample in the diffusion process.

_U_s_a_g_e:

     ddim_scheduler_step(model_output, timestep, sample, schedule, eta, use_clipped_model_output, thresholding, generator, variance_noise, clip_sample, set_alpha_to_one, prediction_type, dtype, device)
     
_A_r_g_u_m_e_n_t_s:

model_output: Numeric array. The output from the diffusion model,
          typically representing predicted noise or the denoised sample
          depending on `prediction_type`.

timestep: Integer. The current timestep in the diffusion process.

  sample: Numeric array. The current noisy sample at timestep `t`.

schedule: List. The DDIM scheduler object containing the necessary
          parameters created from ddim_scheduler_create()

     eta: Numeric. Controls the stochasticity of the process. When
          eta=0, DDIM is deterministic. When eta=1, it's equivalent to
          DDPM. Default: 0

use_clipped_model_output: Logical. Whether to clip the model output
          before computing the sample update. Can improve stability.
          Default: FALSE

thresholding: Logical. Whether to apply thresholding to the output.
          Default: FALSE

generator: An optional random number generator for reproducibility.
          Default: NULL

variance_noise: Optional pre-generated noise for the variance when eta
          > 0. If NULL and eta > 0, noise will be generated. Default:
          NULL

clip_sample: Logical. Whether to clip the sample. Default: FALSE

set_alpha_to_one: Logical. Whether to override the final alpha value to
          1. Used for numerical stability in the final step. Default:
          FALSE

prediction_type: Character. The type of prediction the model outputs.
          Options are: \describe{ \item{"epsilon"}{The model predicts
          the noise} \item{"sample"}{The model predicts the denoised
          sample directly} \item{"v_prediction"}{The model predicts the
          velocity vector (v)} } Default: "epsilon"

   dtype: The data type to use for computations. Default is
          torch_float32().

  device: The device to use for computations. Options are "cpu" and
          "cuda".

_D_e_t_a_i_l_s:

     The DDIM step function implements the core sampling algorithm of
     DDIM described in Song et al. 2020. It computes the previous
     sample x_t-1 given the current sample x_t and the model output.

     The algorithm differs from DDPM by using a non-Markovian diffusion
     process that allows for deterministic sampling and fewer inference
     steps without sacrificing quality.

     When using `prediction_type="epsilon"` (most common), the model
     predicts the noise that was added to create the current noisy
     sample. For `prediction_type="sample"`, the model predicts the
     clean sample directly. The `v_prediction` option implements the
     v-parameterization from Salimans & Ho (2022).

_V_a_l_u_e:

     A list containing: \describe{ \item{`prev_sample`}{The less noisy
     sample at timestep t-1} \item{`pred_original_sample`}{The
     predicted denoised sample} }

_R_e_f_e_r_e_n_c_e_s:

     Song, J., Meng, C., & Ermon, S. (2020). "Denoising Diffusion
     Implicit Models." \url{https://arxiv.org/abs/2010.02502}

     Salimans, T., & Ho, J. (2022). "Progressive Distillation for Fast
     Sampling of Diffusion Models."
     \url{https://arxiv.org/abs/2202.00512}

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     # Perform a denoising step
     result <- ddim_scheduler_step(
       model_output = model_output,
       timestep = timestep,
       sample = sample,
       eta = 0,  # Deterministic sampling
       prediction_type = "epsilon")
     ## End(Not run)
     
```

## detect_text_encoder_architecture

*Detect text encoder architecture from TorchScript file*

```
_D_e_t_e_c_t _t_e_x_t _e_n_c_o_d_e_r _a_r_c_h_i_t_e_c_t_u_r_e _f_r_o_m _T_o_r_c_h_S_c_r_i_p_t _f_i_l_e

_D_e_s_c_r_i_p_t_i_o_n:

     Detect text encoder architecture from TorchScript file

_U_s_a_g_e:

     detect_text_encoder_architecture(torchscript_path)
     
_A_r_g_u_m_e_n_t_s:

torchscript_path: Path to TorchScript encoder .pt file

_V_a_l_u_e:

     List with vocab_size, context_length, embed_dim, num_layers,
     num_heads, mlp_dim

```

## dot-build_fallback_devices

*Build fallback device configuration*

```
_B_u_i_l_d _f_a_l_l_b_a_c_k _d_e_v_i_c_e _c_o_n_f_i_g_u_r_a_t_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     Build fallback device configuration

_U_s_a_g_e:

     .build_fallback_devices(model, strategy)
     
_A_r_g_u_m_e_n_t_s:

   model: Character. Model type.

strategy: Character. Memory strategy.

_V_a_l_u_e:

     Named list of device assignments.

```

## download_component

*Download TorchScript model component for Stable Diffusion*

```
_D_o_w_n_l_o_a_d _T_o_r_c_h_S_c_r_i_p_t _m_o_d_e_l _c_o_m_p_o_n_e_n_t _f_o_r _S_t_a_b_l_e _D_i_f_f_u_s_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     Downloads the required model file (e.g., UNet, decoder, text
     encoder, and tokenizer) for a given Stable Diffusion model into a
     local user-specific data directory. The files will be stored in a
     persistent path returned by [tools::R_user_dir()], typically:
     \itemize{ \item macOS: `~/Library/Application Support/diffuseR/`
     \item Linux: `~/.local/share/R/diffuseR/` \item Windows:
     `C:/Users/<username>/AppData/Local/diffuseR/` } Each model is
     stored in its own subdirectory for better organization. If the
     files already exist, they will not be downloaded again unless
     `overwrite = TRUE`.

_U_s_a_g_e:

     download_component(model_name, component, device, overwrite, show_progress)
     
_A_r_g_u_m_e_n_t_s:

model_name: Character string, the name of the model to download (e.g.,
          `"sd21"`).

component: Character string, the specific model component to download
          (e.g., `"unet"`, `"decoder"`, `"text_encoder"`).

  device: Character string, the device type for which the model is
          intended (e.g., `"cpu"` or `"cuda"`).

overwrite: Logical; if `TRUE`, re-downloads the model files even if
          they already exist.

show_progress: Logical; if `TRUE` (default), displays a progress bar
          during download.

_V_a_l_u_e:

     The local file path to the specific model directory (as a string).

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     model_dir <- download_model("sd21")
     ## End(Not run)
     
```

## download_model

```
_d_o_w_n_l_o_a_d__m_o_d_e_l

_D_e_s_c_r_i_p_t_i_o_n:

     download_model

_U_s_a_g_e:

     download_model(model_name, devices, unet_dtype_str, overwrite, show_progress, download_models)
     
_E_x_a_m_p_l_e_s:

     ## Not run:
     
     model_dir <- download_model("sd21")
     ## End(Not run)
     
```

## filename_from_prompt

```
_f_i_l_e_n_a_m_e__f_r_o_m__p_r_o_m_p_t

_D_e_s_c_r_i_p_t_i_o_n:

     filename_from_prompt

_U_s_a_g_e:

     filename_from_prompt(prompt, datetime)
     
_A_r_g_u_m_e_n_t_s:

  prompt: A character string representing the prompt.

datetime: Logical indicating whether to prepend the current date and
          time to the filename. Default is TRUE.

_V_a_l_u_e:

     A character string representing the generated filename.

_E_x_a_m_p_l_e_s:

     filename_from_prompt("A beautiful sunset over the mountains")
     filename_from_prompt("A beautiful sunset over the mountains", datetime = FALSE)
     
```

## get_required_components

*Get required components for each model type*

```
_G_e_t _r_e_q_u_i_r_e_d _c_o_m_p_o_n_e_n_t_s _f_o_r _e_a_c_h _m_o_d_e_l _t_y_p_e

_D_e_s_c_r_i_p_t_i_o_n:

     This function returns a list of required components for each
     supported model type.

_U_s_a_g_e:

     get_required_components(model_name)
     
_A_r_g_u_m_e_n_t_s:

model_name: A character string representing the name of the model.

_V_a_l_u_e:

     A character vector of required components for the specified model.

```

## img2img

*Image-to-Image Generation with Stable Diffusion*

```
_I_m_a_g_e-_t_o-_I_m_a_g_e _G_e_n_e_r_a_t_i_o_n _w_i_t_h _S_t_a_b_l_e _D_i_f_f_u_s_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     This function generates an image based on an input image and a
     text prompt using the Stable Diffusion model. It allows for
     various configurations such as model name, device, scheduler, and
     more.

_U_s_a_g_e:

     img2img(input_image, prompt, negative_prompt, img_dim, model_name, pipeline, devices, unet_dtype_str, download_models, scheduler, num_inference_steps, strength, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ...)
     
_A_r_g_u_m_e_n_t_s:

input_image: Path to the input image or a tensor representing the
          image.

  prompt: Text prompt to guide the image generation.

negative_prompt: Optional negative prompt to guide the image
          generation.`

 img_dim: Dimension of the output image (default: 512).

model_name: Name of the Stable Diffusion model to use (default:
          "sd21").

pipeline: Optional pre-loaded pipeline. If `NULL`, it will be loaded
          based on `model_name`.

 devices: A named list of devices for each model component (e.g.,
          `list(unet = "cuda", decoder = "cpu", text_encoder = "cpu",
          encoder = "cpu")`).

unet_dtype_str: Optional A character for dtype of the unet component
          (typically "torch_float16" for cuda and "torch_float32" for
          cpu).

download_models: Logical indicating whether to download models if not
          found (default: FALSE).

scheduler: Scheduler to use for the diffusion process (default:
          "ddim").

num_inference_steps: Number of diffusion steps (default: 50).

strength: Strength of the image-to-image transformation (default: 0.8).

guidance_scale: Scale for classifier-free guidance (default: 7.5).

    seed: Random seed for reproducibility (default: NULL).

save_file: Logical indicating whether to save the generated image.

filename: Optional filename for saving the image. If `NULL`, a default
          name is generated.

metadata_path: Path to save metadata (default: NULL).

use_native_decoder: Logical; if TRUE, uses native R torch decoder
          instead of TorchScript. Native decoder has better GPU
          compatibility (especially Blackwell).

use_native_text_encoder: Logical; if TRUE, uses native R torch text
          encoder instead of TorchScript. Native text encoder has
          better GPU compatibility (especially Blackwell).

     ...: Additional arguments for future use.

_V_a_l_u_e:

     An image array and metadata

```

## load_decoder_weights

*Load weights from TorchScript decoder into native decoder*

```
_L_o_a_d _w_e_i_g_h_t_s _f_r_o_m _T_o_r_c_h_S_c_r_i_p_t _d_e_c_o_d_e_r _i_n_t_o _n_a_t_i_v_e _d_e_c_o_d_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Load weights from TorchScript decoder into native decoder

_U_s_a_g_e:

     load_decoder_weights(native_decoder, torchscript_path, verbose)
     
_A_r_g_u_m_e_n_t_s:

native_decoder: Native VAE decoder module

torchscript_path: Path to TorchScript decoder .pt file

 verbose: Print loading progress

_V_a_l_u_e:

     The native decoder with loaded weights (invisibly)

```

## load_model_component

*Load a specific component of a diffusion model*

```
_L_o_a_d _a _s_p_e_c_i_f_i_c _c_o_m_p_o_n_e_n_t _o_f _a _d_i_f_f_u_s_i_o_n _m_o_d_e_l

_D_e_s_c_r_i_p_t_i_o_n:

     Loads a TorchScript model component (UNet, decoder, or text
     encoder) from the local model directory, downloading it first if
     necessary.

_U_s_a_g_e:

     load_model_component(component, model_name, device, unet_dtype_str, download, use_native)
     
_A_r_g_u_m_e_n_t_s:

component: Character string, the component to load: "unet", "decoder",
          or "text_encoder".

model_name: Character string, the name of the model to use.

  device: Character string, the torch device to load the model onto
          ("cpu" or "cuda").

unet_dtype_str: Optional; the data type for the UNet model. If `NULL`,
          defaults to `float32` for CPU and `float16` for CUDA.

download: Logical; if `TRUE` (default), downloads the model if it
          doesn't exist locally.

use_native: Logical; if `TRUE`, uses native R torch modules instead of
          TorchScript. Supported for decoder and text_encoder. Native
          modules have better GPU compatibility.

_V_a_l_u_e:

     A torch model object.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     unet <- load_model_component("unet", "sd21", "cpu")
     ## End(Not run)
     
```

## load_pipeline

*Load a diffusion model pipeline*

```
_L_o_a_d _a _d_i_f_f_u_s_i_o_n _m_o_d_e_l _p_i_p_e_l_i_n_e

_D_e_s_c_r_i_p_t_i_o_n:

     This function loads a diffusion model pipeline consisting of a
     UNet, VAE decoder, and text encoder. It initializes the models and
     sets up the environment for inference.

_U_s_a_g_e:

     load_pipeline(model_name, m2d, i2i, unet_dtype_str, use_native_decoder, use_native_text_encoder, ...)
     
_A_r_g_u_m_e_n_t_s:

model_name: The name of the model to load.

     m2d: A list containing model-to-device mappings and
          configurations.

     i2i: Logical indicating whether to load the encoder for img2img().

unet_dtype_str: A string representing the data type for the UNet model
          (e.g., "float32", "float16").

use_native_decoder: Logical; if TRUE, uses native R torch decoder
          instead of TorchScript. Native decoder has better GPU
          compatibility (especially Blackwell).

use_native_text_encoder: Logical; if TRUE, uses native R torch text
          encoder instead of TorchScript. Native text encoder has
          better GPU compatibility (especially Blackwell).

     ...: Additional arguments passed to the model loading functions.

_V_a_l_u_e:

     An environment containing the loaded models and configuration.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     pipeline <- load_pipeline("my_model", device = "cuda")
     ## End(Not run)
     
```

## load_text_encoder_weights

*Load weights from TorchScript text encoder into native encoder*

```
_L_o_a_d _w_e_i_g_h_t_s _f_r_o_m _T_o_r_c_h_S_c_r_i_p_t _t_e_x_t _e_n_c_o_d_e_r _i_n_t_o _n_a_t_i_v_e _e_n_c_o_d_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Load weights from TorchScript text encoder into native encoder

_U_s_a_g_e:

     load_text_encoder_weights(native_encoder, torchscript_path, verbose)
     
_A_r_g_u_m_e_n_t_s:

native_encoder: Native text encoder module

torchscript_path: Path to TorchScript encoder .pt file

 verbose: Print loading progress

_V_a_l_u_e:

     The native encoder with loaded weights (invisibly)

```

## load_text_encoder2_weights

*Load weights from TorchScript text encoder 2 into native encoder*

```
_L_o_a_d _w_e_i_g_h_t_s _f_r_o_m _T_o_r_c_h_S_c_r_i_p_t _t_e_x_t _e_n_c_o_d_e_r _2 _i_n_t_o _n_a_t_i_v_e _e_n_c_o_d_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Load weights from TorchScript text encoder 2 into native encoder

_U_s_a_g_e:

     load_text_encoder2_weights(native_encoder, torchscript_path, verbose)
     
_A_r_g_u_m_e_n_t_s:

native_encoder: Native text encoder 2 module

torchscript_path: Path to TorchScript encoder .pt file

 verbose: Print loading progress

_V_a_l_u_e:

     The native encoder with loaded weights (invisibly)

```

## models2devices

```
_m_o_d_e_l_s_2_d_e_v_i_c_e_s

_D_e_s_c_r_i_p_t_i_o_n:

     This function sets up the model directory, device configuration,
     and data types for diffusion models. It checks the validity of the
     model name and devices, detects model type, and downloads the
     model if necessary.

_U_s_a_g_e:

     models2devices(model_name, devices, unet_dtype_str, download_models)
     
_A_r_g_u_m_e_n_t_s:

model_name: A character string representing the name of the model to be
          used.

 devices: A character string or a named list specifying the devices for
          different components of the model.

unet_dtype_str: A character string specifying the data type for the
          UNet model.

download_models: Logical indicating whether to download models if they
          are not found.

_V_a_l_u_e:

     A list containing the model directory, model files, device
     configuration, UNet data type, and CPU/CUDA devices.

```

## post_quant_conv

*Post Quant Conv*

```
_P_o_s_t _Q_u_a_n_t _C_o_n_v

_D_e_s_c_r_i_p_t_i_o_n:

     This function applies a quantized convolution operation to an
     input tensor. It is typically used in the context of image post
     processing, particularly in generative models like Stable
     Diffusion XL.

_U_s_a_g_e:

     post_quant_conv(x, dtype, device)
     
_A_r_g_u_m_e_n_t_s:

       x: Input tensor to be processed.

   dtype: Data type for the tensor (e.g., "torch_float16" or
          "torch_float32").

  device: Device on which the tensor is located (e.g., "cpu" or
          "cuda").

_V_a_l_u_e:

     Processed tensor after applying the quantized convolution.

```

## preprocess_image

*Preprocess image for Stable Diffusion*

```
_P_r_e_p_r_o_c_e_s_s _i_m_a_g_e _f_o_r _S_t_a_b_l_e _D_i_f_f_u_s_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     Preprocess image for Stable Diffusion

_U_s_a_g_e:

     preprocess_image(input, device, width, height)
     
_A_r_g_u_m_e_n_t_s:

   input: File path to .jpg or .png, or a 3D array

  device: Target device for torch ("cpu" or "cuda")

   width: Desired width of the output image

  height: Desired height of the output image

_V_a_l_u_e:

     Torch tensor of shape c(1, 3, 512, 512), scaled to c(-1, 1)

```

## quant_conv

*Quant Conv*

```
_Q_u_a_n_t _C_o_n_v

_D_e_s_c_r_i_p_t_i_o_n:

     This function applies a quantized convolution operation to an
     input tensor. It is typically used in the context of image
     processing, particularly in generative models like Stable
     Diffusion.

_U_s_a_g_e:

     quant_conv(x, dtype, device)
     
_A_r_g_u_m_e_n_t_s:

       x: Input tensor to be processed.

   dtype: Data type for the tensor (e.g., "torch_float16" or
          "torch_float32").

  device: Device on which the tensor is located (e.g., "cpu" or
          "cuda").

_V_a_l_u_e:

     Processed tensor after applying the quantized convolution.

```

## quick_gelu

*QuickGELU activation*

```
_Q_u_i_c_k_G_E_L_U _a_c_t_i_v_a_t_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     GELU approximation used by OpenAI CLIP: x * sigmoid(1.702 * x)

_U_s_a_g_e:

     quick_gelu(x)
     
_A_r_g_u_m_e_n_t_s:

       x: Input tensor

```

## save_image

*Save and Display an Image from a Torch Tensor*

```
_S_a_v_e _a_n_d _D_i_s_p_l_a_y _a_n _I_m_a_g_e _f_r_o_m _a _T_o_r_c_h _T_e_n_s_o_r

_D_e_s_c_r_i_p_t_i_o_n:

     Converts a Torch tensor to a normalized RGB image array, saves it
     as a PNG file, and optionally displays it in the RStudio Viewer
     pane using `grid::grid.raster()`.

_U_s_a_g_e:

     save_image(img, save_to, normalize)
     
_A_r_g_u_m_e_n_t_s:

     img: A numeric with shape `[3, H, W]`.

 save_to: File path for the PNG image (default is `"output.png"`).

normalize: Logical; whether to normalize pixel values to `[0, 1]`.
          Default is `TRUE`.

_V_a_l_u_e:

     Invisibly returns the saved file path.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     save_image(output_tensor, "sample.png")
     ## End(Not run)
     
```

## scheduler_add_noise

*Add noise to latents using DDIM scheduler*

```
_A_d_d _n_o_i_s_e _t_o _l_a_t_e_n_t_s _u_s_i_n_g _D_D_I_M _s_c_h_e_d_u_l_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     This function adds noise to the original latents according to the
     DDIM scheduler's diffusion process. It computes the noisy latents
     based on the original latents, noise, and the current timestep.

_U_s_a_g_e:

     scheduler_add_noise(original_latents, noise, timestep, scheduler_obj)
     
_A_r_g_u_m_e_n_t_s:

original_latents: A torch tensor representing the original latents.

   noise: A torch tensor representing the noise to be added.

timestep: An integer representing the current timestep in the diffusion
          process.

scheduler_obj: A list containing the DDIM scheduler parameters,
          including alphas_cumprod and timesteps. The alphas_cumprod
          represents how much of the original signal remains at each
          timestep of the diffusion process.

_D_e_t_a_i_l_s:

     The noise is added according to the standard diffusion forward
     process formula: noised_latents = sqrt(alpha_cumprod) *
     original_latents + sqrt(1-alpha_cumprod) * noise

     Where alpha_cumprod is the cumulative product of (1-beta) values
     up to the specified timestep, with beta being the noise schedule.

_V_a_l_u_e:

     A torch tensor containing the noised latents, which represents the
     original latents with the appropriate amount of noise added for
     the given timestep.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     # Assuming we have latents, noise, and a scheduler
     noised_latents <- scheduler_add_noise(
       original_latents = latents,
       noise = torch::torch_randn_like(latents),
       timestep = scheduler$timesteps[1],
       scheduler_obj = scheduler
     )
     ## End(Not run)
     
```

## setup_dtype

*Set up dtype based on device configuration*

```
_S_e_t _u_p _d_t_y_p_e _b_a_s_e_d _o_n _d_e_v_i_c_e _c_o_n_f_i_g_u_r_a_t_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     Set up dtype based on device configuration

_U_s_a_g_e:

     setup_dtype(devices, unet_dtype_str)
     
_A_r_g_u_m_e_n_t_s:

 devices: A character string or a named list specifying the devices for
          model components.

unet_dtype_str: A character string specifying the data type for the
          UNet model.

_V_a_l_u_e:

     A torch dtype object based on the main computation device.

```

## standardize_devices

*Standardize devices configuration*

```
_S_t_a_n_d_a_r_d_i_z_e _d_e_v_i_c_e_s _c_o_n_f_i_g_u_r_a_t_i_o_n

_D_e_s_c_r_i_p_t_i_o_n:

     This function standardizes the device configuration for model
     components. It checks if the devices parameter is a single string
     or a named list, and fills in missing components with reasonable
     defaults.

_U_s_a_g_e:

     standardize_devices(devices, required_components)
     
_A_r_g_u_m_e_n_t_s:

 devices: A character string or a named list specifying the devices for
          model components.

required_components: A character vector of required components for the
          model.

_V_a_l_u_e:

     A named list of devices for each required component.

```

## text_encoder_native

*Native CLIP Text Encoder*

```
_N_a_t_i_v_e _C_L_I_P _T_e_x_t _E_n_c_o_d_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Native R torch implementation of CLIP text encoder. Replaces
     TorchScript for better GPU compatibility.

_A_r_g_u_m_e_n_t_s:

vocab_size: Vocabulary size (default 49408)

context_length: Maximum sequence length (default 77)

embed_dim: Embedding dimension

num_layers: Number of transformer layers

num_heads: Number of attention heads

 mlp_dim: MLP hidden dimension

_V_a_l_u_e:

     An nn_module representing the text encoder

```

## text_encoder2_native

*Native CLIP Text Encoder 2 (OpenCLIP ViT-bigG for SDXL)*

```
_N_a_t_i_v_e _C_L_I_P _T_e_x_t _E_n_c_o_d_e_r _2 (_O_p_e_n_C_L_I_P _V_i_T-_b_i_g_G _f_o_r _S_D_X_L)

_D_e_s_c_r_i_p_t_i_o_n:

     Native R torch implementation of OpenCLIP text encoder used in
     SDXL. Returns both hidden states and pooled output.

_A_r_g_u_m_e_n_t_s:

vocab_size: Vocabulary size (default 49408)

context_length: Maximum sequence length (default 77)

embed_dim: Embedding dimension (default 1280)

num_layers: Number of transformer layers (default 32)

num_heads: Number of attention heads (default 20)

 mlp_dim: MLP hidden dimension (default 5120)

_V_a_l_u_e:

     An nn_module representing the text encoder

```

## txt2img_sd21

*Generate an image from a text prompt using a diffusion pipeline*

```
_G_e_n_e_r_a_t_e _a_n _i_m_a_g_e _f_r_o_m _a _t_e_x_t _p_r_o_m_p_t _u_s_i_n_g _a _d_i_f_f_u_s_i_o_n _p_i_p_e_l_i_n_e

_D_e_s_c_r_i_p_t_i_o_n:

     This function generates an image based on a text prompt using the
     Stable Diffusion model. It allows for various configurations such
     as model name, device, scheduler, and more.

_U_s_a_g_e:

     txt2img_sd21(prompt, negative_prompt, img_dim, pipeline, devices, unet_dtype_str, download_models, scheduler, timesteps, initial_latents, num_inference_steps, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ...)
     
_A_r_g_u_m_e_n_t_s:

  prompt: A character string prompt describing the image to generate.

negative_prompt: Optional negative prompt to guide the generation.

 img_dim: Dimension of the output image (e.g., 512 for 512x512).

pipeline: Optional A pre-loaded diffusion pipeline. If `NULL`, it will
          be loaded based on the model name and devices.

 devices: A named list of devices for each model component (e.g.,
          `list(unet = "cuda", decoder = "cpu", text_encoder =
          "cpu")`).

unet_dtype_str: Optional A character for dtype of the unet component
          (typically "float16" for cuda and "float32" for cpu; float32
          is available for cuda).

download_models: Logical indicating whether to download the model files
          if they are not found.

scheduler: Scheduler to use (e.g., `"ddim"`, `"euler"`).

timesteps: Optional A vector of timesteps to use.

initial_latents: Optional initial latents for the diffusion process.

num_inference_steps: Number of inference steps to run.

guidance_scale: Scale for classifier-free guidance (typically 7.5).

    seed: Optional seed for reproducibility.

save_file: Logical indicating whether to save the generated image.

filename: Optional filename for saving the image. If `NULL`, a default
          name is generated.

metadata_path: Optional file path to save metadata.

use_native_decoder: Logical; if TRUE, uses native R torch decoder
          instead of TorchScript. Native decoder has better GPU
          compatibility (especially Blackwell).

use_native_text_encoder: Logical; if TRUE, uses native R torch text
          encoder instead of TorchScript. Native text encoder has
          better GPU compatibility (especially Blackwell).

     ...: Additional parameters passed to the diffusion process.

_V_a_l_u_e:

     An image array and metadata

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     img <- txt2img("a cat wearing sunglasses in space", device = "cuda")
     ## End(Not run)
     
```

## txt2img_sdxl

*Generate an image from a text prompt using a diffusion pipeline*

```
_G_e_n_e_r_a_t_e _a_n _i_m_a_g_e _f_r_o_m _a _t_e_x_t _p_r_o_m_p_t _u_s_i_n_g _a _d_i_f_f_u_s_i_o_n _p_i_p_e_l_i_n_e

_D_e_s_c_r_i_p_t_i_o_n:

     Generate an image from a text prompt using a diffusion pipeline

_U_s_a_g_e:

     txt2img_sdxl(prompt, negative_prompt, img_dim, pipeline, devices, unet_dtype_str, download_models, scheduler, timesteps, initial_latents, num_inference_steps, guidance_scale, seed, save_file, filename, metadata_path, use_native_decoder, use_native_text_encoder, ...)
     
_A_r_g_u_m_e_n_t_s:

  prompt: A character string prompt describing the image to generate.

negative_prompt: Optional negative prompt to guide the generation.

 img_dim: Dimension of the output image (e.g., 512 for 512x512).

pipeline: Optional A pre-loaded diffusion pipeline. If `NULL`, it will
          be loaded based on the model name and devices.

 devices: A named list of devices for each model component (e.g.,
          `list(unet = "cuda", decoder = "cpu", text_encoder =
          "cpu")`).

unet_dtype_str: Optional A character for dtype of the unet component
          (typically "float16" for cuda and "float32" for cpu; float32
          is available for cuda).

download_models: Logical indicating whether to download the model files
          if they are not found.

scheduler: Scheduler to use (e.g., `"ddim"`, `"euler"`).

timesteps: Optional A vector of timesteps to use.

initial_latents: Optional initial latents for the diffusion process.

num_inference_steps: Number of inference steps to run.

guidance_scale: Scale for classifier-free guidance (typically 7.5).

    seed: Optional seed for reproducibility.

save_file: Logical indicating whether to save the generated image.

filename: Optional filename for saving the image. If `NULL`, a default
          name is generated.

metadata_path: Optional file path to save metadata.

use_native_decoder: Logical; if TRUE, uses native R torch decoder
          instead of TorchScript. Native decoder has better GPU
          compatibility (especially Blackwell).

use_native_text_encoder: Logical; if TRUE, uses native R torch text
          encoder instead of TorchScript. Native text encoder has
          better GPU compatibility (especially Blackwell).

     ...: Additional parameters passed to the diffusion process.

_V_a_l_u_e:

     An image array and metadata

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     img <- txt2img("a cat wearing sunglasses in space", device = "cuda")
     ## End(Not run)
     
```

## txt2img

*Generate an image from a text prompt using a diffusion pipeline*

```
_G_e_n_e_r_a_t_e _a_n _i_m_a_g_e _f_r_o_m _a _t_e_x_t _p_r_o_m_p_t _u_s_i_n_g _a _d_i_f_f_u_s_i_o_n _p_i_p_e_l_i_n_e

_D_e_s_c_r_i_p_t_i_o_n:

     Generate an image from a text prompt using a diffusion pipeline

_U_s_a_g_e:

     txt2img(prompt, model_name, ...)
     
_A_r_g_u_m_e_n_t_s:

  prompt: A character string prompt describing the image to generate.

model_name: Name of the model to use (e.g., `"sd21"`).

     ...: Additional parameters passed to the diffusion process.

_V_a_l_u_e:

     A tensor or image object, depending on implementation.

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     img <- txt2img("a cat wearing sunglasses in space", device = "cuda")
     ## End(Not run)
     
```

## vae_decoder_native

*Native VAE Decoder*

```
_N_a_t_i_v_e _V_A_E _D_e_c_o_d_e_r

_D_e_s_c_r_i_p_t_i_o_n:

     Native R torch implementation of the SDXL VAE decoder. Replaces
     TorchScript decoder for better GPU compatibility.

_A_r_g_u_m_e_n_t_s:

latent_channels: Number of latent channels (default 4)

out_channels: Number of output channels (default 3 for RGB)

_V_a_l_u_e:

     An nn_module representing the VAE decoder

_E_x_a_m_p_l_e_s:

     ## Not run:
     
     decoder <- vae_decoder_native()
     load_decoder_weights(decoder, "path/to/decoder.pt")
     latents <- torch::torch_randn(c(1, 4, 64, 64))
     image <- decoder(latents)
     ## End(Not run)
     
```

## VAEAttentionBlock

*VAE Attention Block*

```
_V_A_E _A_t_t_e_n_t_i_o_n _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     Self-attention for VAE mid block

_A_r_g_u_m_e_n_t_s:

channels: Number of channels

```

## VAEMidBlock

*VAE Mid Block*

```
_V_A_E _M_i_d _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     VAE Mid Block

_A_r_g_u_m_e_n_t_s:

channels: Number of channels

```

## VAEResnetBlock

*VAE ResNet Block*

```
_V_A_E _R_e_s_N_e_t _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     VAE ResNet Block

_A_r_g_u_m_e_n_t_s:

in_channels: Input channels

out_channels: Output channels

```

## VAEUpBlock

*VAE Up Block*

```
_V_A_E _U_p _B_l_o_c_k

_D_e_s_c_r_i_p_t_i_o_n:

     VAE Up Block

_A_r_g_u_m_e_n_t_s:

in_channels: Input channels

out_channels: Output channels

num_resnets: Number of resnet blocks (default 3)

add_upsample: Whether to add upsampler

```

