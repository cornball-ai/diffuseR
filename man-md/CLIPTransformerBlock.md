<!-- diffuseR::CLIPTransformerBlock -->
<!-- Generated by fyi::use_fyi_docs() -->

### CLIP Transformer Block

#### Description

Pre-norm transformer block with attention and MLP (HuggingFace style)

#### Arguments

- **`embed_dim`**: Embedding dimension
- **`num_heads`**: Number of attention heads
- **`mlp_dim`**: MLP hidden dimension
- **`gelu_type`**: GELU variant: "tanh", "quick", or "exact"

